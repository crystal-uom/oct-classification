{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.1+cpu\n",
      "torchvision version: 0.18.1+cpu\n"
     ]
    }
   ],
   "source": [
    "# Continue with regular imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")\n",
    "\n",
    "# Try to get torchinfo, install it if it doesn't work\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and Output Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised_oct_training_dataset = \"D:/VIP Cup/Output dataset/denoised_pkl_dataset\"\n",
    "results = \"D:/VIP Cup/Output dataset/classification_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for open a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def open_pickle(path_to_pkl):\n",
    "\n",
    "    with open(path_to_pkl, 'rb') as file:\n",
    "        loaded_vol_oct = pickle.load(file)\n",
    "\n",
    "    return loaded_vol_oct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_test_data(dataset, test_ratio: float):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and testing datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: The dataset to be split.\n",
    "    - test_ratio: The ratio of the testing dataset size to the total dataset size.\n",
    "\n",
    "    Returns:\n",
    "    - train_dataset: The training dataset as paths.\n",
    "    - test_dataset: The testing dataset as paths.\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(42)\n",
    "    train_dataset, test_dataset = [],[]\n",
    "\n",
    "    labels = os.listdir(dataset)\n",
    "    for label in labels:\n",
    "        label_path = os.path.join(dataset, label)\n",
    "        vol_images = os.listdir(label_path)\n",
    "        train_images, test_images = train_test_split(vol_images, test_size=test_ratio, random_state=42)\n",
    "        train_image_count = len(train_images)\n",
    "        test_image_count = len(test_images)\n",
    "\n",
    "        for train_image in train_images:\n",
    "            train_dataset.append(os.path.join(label_path, train_image))\n",
    "        for test_image in test_images:\n",
    "            test_dataset.append(os.path.join(label_path, test_image))\n",
    "\n",
    "        print(f\"class {label} has {train_image_count} training images and {test_image_count} testing images.\")\n",
    "        \n",
    "\n",
    "    return train_dataset, test_dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 has 34 training images and 8 testing images.\n",
      "class 1 has 24 training images and 6 testing images.\n",
      "class 2 has 23 training images and 5 testing images.\n",
      "\n",
      "Number of training samples: 81\n",
      "Number of testing samples: 19\n"
     ]
    }
   ],
   "source": [
    "train_dataset_path, test_dataset_path, classes = split_train_test_data(denoised_oct_training_dataset, test_ratio=0.1667)\n",
    "print()\n",
    "print(f\"Number of training samples: {len(train_dataset_path)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (37).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (15).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (34).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (20).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (25).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (18).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (23).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (4).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (24).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (42).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (41).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (1).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (31).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (14).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (2).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (8).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (10).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (38).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (29).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (11).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (39).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (9).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (12).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (5).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (30).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (40).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (19).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (3).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (26).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (28).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (16).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (22).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (35).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (6).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (8).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (4).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (20).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (1).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (13).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (24).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (14).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (21).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (2).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (3).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (10).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (11).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (5).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (12).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (29).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (6).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (26).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (9).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (28).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (16).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (19).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (22).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (27).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (15).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (20).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (25).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (4).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (2).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (21).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (23).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (10).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (13).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (14).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (11).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (24).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (5).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (12).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (8).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (6).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (26).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (9).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (28).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (16).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (19).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (22).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (27).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (15).pkl']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (32).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (21).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (17).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (33).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (13).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (7).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (27).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\0\\\\RawDataQA (36).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (7).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (23).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (30).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (25).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (17).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\1\\\\RawDataQA-1 (18).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (18).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (7).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (17).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (3).pkl',\n",
       " 'D:/VIP Cup/Output dataset/denoised_pkl_dataset\\\\2\\\\RawDataQA-2 (1).pkl']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the volumetric dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vol_datasets(dataset):\n",
    "    \"\"\"load pickle files as a numpy array associated with their labels and return them as a dataset\"\"\"\n",
    "    \n",
    "    dataset_list = []\n",
    "    for path in dataset:\n",
    "        vol_oct = open_pickle(path)\n",
    "        label = os.path.basename(os.path.dirname(path))\n",
    "        dataset_list.append((vol_oct, label))\n",
    "    return dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vol_dataset = create_vol_datasets(train_dataset_path)\n",
    "test_vol_dataset = create_vol_datasets(test_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 19)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vol_dataset), len(test_vol_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 2D sclices of dataset from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2D_dataset(vol_dataset):\n",
    "    \"\"\"create a 2D dataset from the 3D dataset\"\"\"\n",
    "    \n",
    "    dataset_2D = []\n",
    "    for vol_oct, label in vol_dataset:\n",
    "        m = vol_oct.shape[2]\n",
    "        for i in range(m):\n",
    "            dataset_2D.append((vol_oct[:,:,i], label))\n",
    "    return dataset_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_2D_dataset(train_vol_dataset)\n",
    "test_dataset = create_2D_dataset(test_vol_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of 2D array: (300, 300)\n",
      "Shape of 3D array: (300, 300, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create or load your 2D NumPy array\n",
    "array_2d = train_dataset[0][0]  # A 2D array of size 300x300\n",
    "\n",
    "# Convert the 2D array to a 3D array with shape (300, 300, 1)\n",
    "array_3d = np.expand_dims(array_2d, axis=-1)\n",
    "\n",
    "# Check the shape of the resulting array\n",
    "print(\"Shape of 2D array:\", array_2d.shape)\n",
    "print(\"Shape of 3D array:\", array_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 14698\n",
      "Number of testing samples: 3340\n",
      "Number of total samples: 18038\n",
      "Ratio of the testing dataset size to the total dataset size: 0.1852\n",
      "Sample shape of the training data: (300, 300)\n"
     ]
    }
   ],
   "source": [
    "training_cases = len(train_dataset)\n",
    "testing_cases = len(test_dataset)\n",
    "total_cases = training_cases + testing_cases\n",
    "test_ratio = testing_cases / total_cases\n",
    "\n",
    "print(f\"Number of training samples: {training_cases}\")\n",
    "print(f\"Number of testing samples: {testing_cases}\")\n",
    "print(f\"Number of total samples: {total_cases}\")\n",
    "print(f\"Ratio of the testing dataset size to the total dataset size: {test_ratio:.4f}\")\n",
    "print(f\"Sample shape of the training data: {train_dataset[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'Normal', '1': 'Diabetes', '2': 'non-Diabetes'}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = {classes[0]: \"Normal\", classes[1]: \"Diabetes\", classes[2]: \"non-Diabetes\"}\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are intending to use ResNet-101 for the classification task. We will use the pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet101_Weights.IMAGENET1K_V2"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get a set of pretrained model weights\n",
    "\n",
    "# Get the weights\n",
    "weights = torchvision.models.ResNet101_Weights.DEFAULT\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[232]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_transforms = weights.transforms()\n",
    "auto_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create manual transforms to include the RGB conversion also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "class GrayscaleToRGB:\n",
    "    \"\"\"convert a grayscale image to an RGB image\"\"\"\n",
    "    def __call__(self, image):\n",
    "        if image.ndim == 2:\n",
    "            img = np.stack((image,) * 3, axis=-1)\n",
    "            img2 = Image.fromarray(img)\n",
    "            print(f\"image shape: {img2.size}\")\n",
    "        return img2\n",
    "\n",
    "crop_size = 300\n",
    "resize_size = 232\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "interpolation = InterpolationMode.BILINEAR\n",
    "\n",
    "final_transform = transforms.Compose([\n",
    "    GrayscaleToRGB(),\n",
    "    transforms.CenterCrop(crop_size),\n",
    "    transforms.Resize(resize_size, interpolation=interpolation),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3602106 , 0.36963236, 0.40482548, ..., 0.36588416, 0.38754609,\n",
       "        0.39784697],\n",
       "       [0.36010915, 0.36944386, 0.40498698, ..., 0.36151809, 0.38983473,\n",
       "        0.39774218],\n",
       "       [0.36048073, 0.37109655, 0.40413761, ..., 0.360349  , 0.39183092,\n",
       "        0.39884546],\n",
       "       ...,\n",
       "       [0.50629044, 0.4697457 , 0.48424375, ..., 0.51213938, 0.51351953,\n",
       "        0.51544142],\n",
       "       [0.51053566, 0.47276598, 0.48769131, ..., 0.51471663, 0.51600945,\n",
       "        0.51488155],\n",
       "       [0.51183861, 0.47449845, 0.49121377, ..., 0.51666921, 0.51870942,\n",
       "        0.51389706]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_dataset[0][0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 3), <f8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:2992\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2991\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2992\u001b[0m     mode, rawmode \u001b[38;5;241m=\u001b[39m \u001b[43m_fromarray_typemap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypekey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2993\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyError\u001b[0m: ((1, 1, 3), '<f8')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[225], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m a\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "Cell \u001b[1;32mIn[223], line 12\u001b[0m, in \u001b[0;36mGrayscaleToRGB.__call__\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     11\u001b[0m     img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack((image,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m     img2 \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg2\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img2\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:2994\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2992\u001b[0m         mode, rawmode \u001b[38;5;241m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[0;32m   2993\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2994\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m typekey) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   2995\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2996\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m mode\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 3), <f8"
     ]
    }
   ],
   "source": [
    "a = final_transform(sample)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<1,-1,-1>,struct cv::impl::A0x59191d0d::Set<3,4,-1>,struct cv::impl::A0x59191d0d::Set<0,2,5>,4>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[226], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m sample \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Apply the final transformation\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m a\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "Cell \u001b[1;32mIn[226], line 12\u001b[0m, in \u001b[0;36mGrayscaleToRGB.__call__\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 12\u001b[0m         rgb_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_GRAY2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rgb_image\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<1,-1,-1>,struct cv::impl::A0x59191d0d::Set<3,4,-1>,struct cv::impl::A0x59191d0d::Set<0,2,5>,4>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "class GrayscaleToRGB:\n",
    "    \"\"\"Convert a grayscale image to an RGB image.\"\"\"\n",
    "    def __call__(self, image):\n",
    "        if image.ndim == 2:\n",
    "            rgb_image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        return rgb_image\n",
    "\n",
    "crop_size = 300\n",
    "resize_size = 232\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "interpolation = InterpolationMode.BILINEAR\n",
    "\n",
    "final_transform = transforms.Compose([\n",
    "    GrayscaleToRGB(),\n",
    "    transforms.CenterCrop(crop_size),\n",
    "    transforms.Resize(resize_size, interpolation=interpolation),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Assuming `sample` is a numpy array representing a grayscale image\n",
    "sample = train_dataset[0][0]\n",
    "\n",
    "# Apply the final transformation\n",
    "a = final_transform(sample)\n",
    "a\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
